{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C with PPO training in the unity environment with the car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import zip_longest\n",
    "\n",
    "import typing\n",
    "from typing import NamedTuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from mlagents.trainers.demo_loader import load_demonstration\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple, DecisionSteps, TerminalSteps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unity environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_env = UnityEnvironment()\n",
    "unity_env.reset()\n",
    "name = list(unity_env.behavior_specs.keys())[0]\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observation(NamedTuple):\n",
    "    \"\"\"\n",
    "    Environment observation tuple\n",
    "    \"\"\"\n",
    "    camera: np.ndarray\n",
    "    ifr_r: np.ndarray\n",
    "    ifr_l: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityEnvWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper of unity environment for gym like interface\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unity_env: UnityEnvironment):\n",
    "        self._unity_env = unity_env\n",
    "\n",
    "        self._unity_env.reset()\n",
    "\n",
    "        self._name = list(self._unity_env.behavior_specs.keys())[0]\n",
    "        self._group_spec = self._unity_env.behavior_specs[self._name]\n",
    "\n",
    "        self._done = False\n",
    "\n",
    "    def _get_step(self) -> typing.Union[DecisionSteps, TerminalSteps]:\n",
    "        decision_step, terminal_step = self._unity_env.get_steps(self._name)\n",
    "\n",
    "        if len(terminal_step) != 0:\n",
    "            self._done = True\n",
    "            return terminal_step\n",
    "        else:\n",
    "            return decision_step\n",
    "\n",
    "    def _get_step_observation(self, step: typing.Union[DecisionSteps, TerminalSteps]) -> Observation:\n",
    "        return Observation(\n",
    "            camera=step.obs[0],\n",
    "            ifr_l=step.obs[1],\n",
    "            ifr_r=step.obs[2]\n",
    "        )\n",
    "\n",
    "    def _process_step(self, step: typing.Union[DecisionSteps, TerminalSteps]) -> tuple[Observation, float, bool, str]:\n",
    "        return self._get_step_observation(step), step.reward[0], isinstance(step, TerminalSteps), \"\"\n",
    "    \n",
    "    def reset(self) -> Observation:\n",
    "        self._unity_env.reset()\n",
    "        self._done = False\n",
    "\n",
    "        step = self._get_step()\n",
    "        return self._get_step_observation(step)\n",
    "\n",
    "    def step(self, actions: ActionTuple) -> tuple[Observation, float, bool, str]:\n",
    "        if self._done:\n",
    "            raise ValueError(\"Actions passed to the done env\")\n",
    "\n",
    "        self._unity_env.set_actions(self._name, actions)\n",
    "        self._unity_env.step()\n",
    "\n",
    "        step = self._get_step()\n",
    "\n",
    "        return self._process_step(step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvWrapper(unity_env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that environment responds to the actions and provides the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "rewards = []\n",
    "\n",
    "np_action = np.array([[1, 0]])\n",
    "action = ActionTuple(np_action)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "while not done:\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_after_conv(in_size: int, kernel_size: int, stride: int):\n",
    "    return (in_size - kernel_size) // stride + 1\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Camera observation encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, im_size: tuple[int, int, int], out_features: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.im_size = im_size\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(im_size[0], 32, 3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Calculates width and height of output image\n",
    "        flat_size_w = reduce(lambda x, _: size_after_conv(x, 3, 2), range(3), im_size[1])\n",
    "        flat_size_h = reduce(lambda x, _: size_after_conv(x, 3, 2), range(3), im_size[2])\n",
    "        self.flat_size = flat_size_w * flat_size_h * 64\n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(self.flat_size, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, out_features)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = X.view(-1, *self.im_size)\n",
    "\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.conv3(X)\n",
    "\n",
    "        X = X.view(-1, self.flat_size)\n",
    "\n",
    "        X = self.linear1(X)\n",
    "        X = self.linear2(X)\n",
    "\n",
    "        return self.out(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes output of several encodings into one \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *in_sizes: int, out_features: int, hidden_size: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = sum(in_sizes)\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, *embeds: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        X = torch.cat(embeds, dim=1)\n",
    "\n",
    "        return self.linear(self.hidden(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    State encoder for actor and critic models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *encoders: nn.Module, state_encoder: nn.Module, state_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.state_encoder = state_encoder\n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTMCell(input_size=256, hidden_size=128)\n",
    "        self.hid = torch.zeros(1, 128)\n",
    "        self.cell = torch.zeros(1, 128)\n",
    "\n",
    "        self.head = nn.Linear(128, state_size)\n",
    "\n",
    "    @property\n",
    "    def memory(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (self.hid.detach(), self.cell.detach())\n",
    "    \n",
    "    def reset(self, memory: typing.Optional[tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        self.hid, self.cell = memory if memory else (torch.zeros(1, 128), torch.zeros(1, 128))\n",
    "        \n",
    "    def forward(self, X: torch.Tensor, memory: typing.Optional[tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "\n",
    "        X = torch.cat(\n",
    "            [self.state_encoder(\n",
    "                *(f(x) for f, x in zip_longest(self.encoders, X_, fillvalue=lambda x: x))\n",
    "            )\n",
    "            for X_ in X]\n",
    "        )\n",
    "\n",
    "        X = self.linear1(X)\n",
    "        X = self.linear2(X)\n",
    "\n",
    "        if memory is not None:\n",
    "            self.hid, self.cell = memory\n",
    "\n",
    "        X, _ = self.hid, self.cell = self.lstm(X, self.memory)\n",
    "\n",
    "        return self.head(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder: nn.Module, state_size: int, n_actions: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.alpha_head = nn.Sequential(\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.beta_head = nn.Sequential(\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.Tensor, memory: typing.Optional[tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "\n",
    "        X = self.encoder(X, memory)\n",
    "\n",
    "        X = self.linear1(X)\n",
    "        X = self.linear2(X)\n",
    "\n",
    "        return self.alpha_head(X), self.beta_head(X)\n",
    "\n",
    "    def get_policy(self, state, memory: typing.Optional[tuple[torch.Tensor, torch.Tensor]] = None) -> dist.Beta:\n",
    "        alpha, beta = self(state, memory)\n",
    "\n",
    "        # Beta policy is used to sample values from 0 to 1\n",
    "        return dist.Beta(alpha, beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, state_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, memory: typing.Optional[tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "\n",
    "        X = self.encoder(X, memory)\n",
    "\n",
    "        X = self.linear1(X)\n",
    "        X = self.linear2(X)\n",
    "        X = self.linear3(X)\n",
    "\n",
    "        return self.value(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_actor(n_action: int) -> Actor:\n",
    "    image_encoder = ImageEncoder((3, 256, 256), 128)\n",
    "    state_encoder = StateEncoder(128, 1, 1, out_features=256)\n",
    "    encoder = Encoder(image_encoder, state_encoder=state_encoder, state_size=256)\n",
    "\n",
    "    actor = Actor(encoder=encoder, state_size=256, n_actions=n_action)\n",
    "\n",
    "    return actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_critic() -> Critic:\n",
    "    image_encoder = ImageEncoder((3, 256, 256), 128)\n",
    "    state_encoder = StateEncoder(128, 1, 1, out_features=256)\n",
    "    encoder = Encoder(image_encoder, state_encoder=state_encoder, state_size=256)\n",
    "\n",
    "    critic = Critic(encoder=encoder, state_size=256)\n",
    "\n",
    "    return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: nn.Module, model_name: str, episode: int):\n",
    "    model.load_state_dict(torch.load(f'models/{model_name}/{model_name}-{episode:04}.torch'))\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, model_name: str, episode: int):\n",
    "    torch.save(model.state_dict(), f'models/{model_name}/{model_name}-{episode:>04}.torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = make_actor(2)\n",
    "critic = make_critic()\n",
    "\n",
    "load_model(actor, \"actor\", 1090)\n",
    "load_model(critic, \"critic\", 1090)\n",
    "\n",
    "actor_old = make_actor(2)\n",
    "actor_old.load_state_dict(actor.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(reduce(lambda x, a: x * a, parameter.size(), 1) for parameter in model.parameters())\n",
    "\n",
    "count_parameters(actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Self = typing.TypeVar(\"Self\", bound=\"Buffer\")\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    \"\"\"\n",
    "    Base class for buffer dataclasses\n",
    "    \"\"\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.reward)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "\n",
    "    def __getitem__(self, index: typing.Union[int, slice]) -> Self:\n",
    "        return type(self)(**{field: values[index] for field, values in self.__dict__.items()})\n",
    "\n",
    "    def add(self, **kwargs) -> None:\n",
    "        for key, value in kwargs.items():\n",
    "            self.__dict__[key].append(value)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        for l in self.__dict__.values():\n",
    "            del l[:]\n",
    "\n",
    "    def batches(self, batch_size) -> typing.Generator[Self, None, None]:\n",
    "        for i in range(0, len(self), batch_size):\n",
    "            yield self[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_obs(obs: typing.Iterable[np.ndarray]) -> typing.Iterable[torch.Tensor]:\n",
    "    return [torch.from_numpy(o).float() for o in obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imitation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ImitationBuffer(Buffer):\n",
    "    \n",
    "    observation: list[list[torch.Tensor]] = field(default_factory=list)\n",
    "    reward: list[float] = field(default_factory=list)\n",
    "    state_value: list[float] = field(default_factory=list)\n",
    "\n",
    "    action: list[torch.Tensor] = field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 3e-4\n",
    "GAMMA = 0.99\n",
    "N_EPOCHS = 3000\n",
    "MINIBATCH_SIZE = 20\n",
    "\n",
    "ENTROPY_K = 0.01\n",
    "\n",
    "actor_bc_optim = optim.Adam(actor.parameters(), LR)\n",
    "critic_bc_optim = optim.Adam(critic.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec, info_list, total = load_demonstration(\"demos/record.demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: list[ImitationBuffer] = []\n",
    "\n",
    "buffer = ImitationBuffer()\n",
    "\n",
    "for record in info_list:\n",
    "\n",
    "    observations = record.agent_info.observations\n",
    "    observation = Observation(\n",
    "        camera=np.array(Image.open(BytesIO(observations[0].compressed_data))),\n",
    "        ifr_l=np.array([observations[1].float_data.data]),\n",
    "        ifr_r=np.array([observations[2].float_data.data])\n",
    "    )\n",
    "\n",
    "    action = torch.FloatTensor(record.action_info.continuous_actions)\n",
    "    action[1] = (action[1] + 1) / 2\n",
    "    buffer.add(\n",
    "        observation=prepare_obs(observation),\n",
    "        reward=record.agent_info.reward,\n",
    "        action=action\n",
    "    )\n",
    "\n",
    "    if record.agent_info.done:\n",
    "\n",
    "        r = 0\n",
    "        for reward in buffer.reward:\n",
    "            r = reward + GAMMA * r\n",
    "            buffer.add(state_value=r)\n",
    "\n",
    "        buffer.state_value.reverse()\n",
    "\n",
    "        dataset.append(buffer)\n",
    "        buffer = ImitationBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bc(batch: ImitationBuffer, values: torch.Tensor, logprobs: torch.Tensor) -> None:\n",
    "    true_values = torch.tensor(batch.state_value).unsqueeze(1)\n",
    "\n",
    "    critic_loss = F.mse_loss(values, true_values)\n",
    "    actor_loss = -logprobs.mean()\n",
    "\n",
    "    critic_bc_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_bc_optim.step()\n",
    "\n",
    "    actor_bc_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_bc_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_value_logprob(batch: ImitationBuffer) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    actor_memory = None\n",
    "    critic_memory = None\n",
    "\n",
    "    values = []\n",
    "    logprobs = []\n",
    "    \n",
    "    for item in batch:\n",
    "        obs = item.observation\n",
    "\n",
    "        policy = actor.get_policy([obs], actor_memory)\n",
    "        action = item.action\n",
    "        action[1] = (action[1] + 1) / 2\n",
    "        logprobs.append(policy.log_prob(item.action) + ENTROPY_K * policy.entropy())\n",
    "\n",
    "        values.append(critic([obs], critic_memory))\n",
    "\n",
    "        critic_memory = critic.encoder.memory\n",
    "        actor_memory = actor.encoder.memory\n",
    "\n",
    "    values = torch.stack(values).squeeze(1)\n",
    "    logprobs = torch.stack(logprobs).squeeze(1)\n",
    "\n",
    "    return values, logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tepoch = trange(N_EPOCHS)\n",
    "for epoch in tepoch:\n",
    "\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    for trajectory in dataset:\n",
    "\n",
    "        actor.encoder.reset()\n",
    "        critic.encoder.reset()    \n",
    "        \n",
    "        for batch in trajectory.batches(MINIBATCH_SIZE):\n",
    "\n",
    "            train_bc(batch, *sample_value_logprob(batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(actor, \"actor\", 1090)\n",
    "save_model(critic, \"critic\", 1090)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplayBuffer(Buffer):    \n",
    "\n",
    "    observation: list[list[torch.Tensor]] = field(default_factory=list)\n",
    "\n",
    "    actor_memory: list[tuple[torch.Tensor, torch.Tensor]] = field(default_factory=list)\n",
    "    critic_memory: list[tuple[torch.Tensor, torch.Tensor]] = field(default_factory=list)\n",
    "\n",
    "    action: list[torch.Tensor] = field(default_factory=list)\n",
    "    old_logprob: list[torch.Tensor] = field(default_factory=list)\n",
    "    \n",
    "    is_terminal: list[bool] = field(default_factory=list)\n",
    "    reward: list[float] = field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 3e-4\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "N_EPISODES = 3000\n",
    "N_STEPS = 400\n",
    "EPS_CLIP = 0.2\n",
    "K_EPOCHS = 10\n",
    "T_HORIZON = 5\n",
    "\n",
    "ENTROPY_K = 0.01\n",
    "\n",
    "buffer = ReplayBuffer()\n",
    "\n",
    "actor_optim = optim.Adam(actor.parameters(), LR)\n",
    "critic_optim = optim.Adam(critic.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_memory(memory: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    hid, cell = zip(*memory)\n",
    "    hid = torch.stack(hid).squeeze(1)\n",
    "    cell = torch.stack(cell).squeeze(1)\n",
    "\n",
    "    return hid, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(buffer: ReplayBuffer, Qval):\n",
    "\n",
    "    Qvals = np.zeros((len(buffer), 1))\n",
    "\n",
    "    for i, item in enumerate(reversed(buffer)):\n",
    "        Qval = item.reward + (1 - item.is_terminal) * LAMBDA * GAMMA * Qval\n",
    "        Qvals[len(buffer) - i - 1] = Qval\n",
    "\n",
    "    Qvals = torch.tensor(Qvals)\n",
    "    old_logprobs = torch.stack(buffer.old_logprob)\n",
    "\n",
    "    for _ in range(K_EPOCHS):\n",
    "\n",
    "        values = critic(buffer.observation, prepare_memory(buffer.critic_memory))\n",
    "\n",
    "        advantage = Qvals - values\n",
    "\n",
    "        policy = actor.get_policy(buffer.observation, prepare_memory(buffer.actor_memory))\n",
    "        logprobs = policy.log_prob(torch.stack(buffer.action).squeeze())\n",
    "\n",
    "        ratios = torch.exp(logprobs - old_logprobs)\n",
    "\n",
    "        surr1 = ratios * advantage.detach()\n",
    "        surr2 = torch.clamp(ratios, 1 - EPS_CLIP, 1 + EPS_CLIP) * advantage.detach()\n",
    "\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "        actor_loss = -torch.min(surr1, surr2).mean() + ENTROPY_K * policy.entropy().mean()\n",
    "\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()\n",
    "\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optim.step()\n",
    "    \n",
    "    actor_old.load_state_dict(actor.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lengths = []\n",
    "all_rewards = []\n",
    "\n",
    "tepisodes = trange(N_EPISODES)\n",
    "for episode in tepisodes:\n",
    "\n",
    "    actor_memory = critic_memory = None\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    steps = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        buffer.clear()\n",
    "\n",
    "        actor_old.encoder.reset(actor_memory)\n",
    "        critic.encoder.reset(critic_memory)\n",
    "\n",
    "        for t in range(T_HORIZON):\n",
    "            obs = prepare_obs(obs)\n",
    "\n",
    "            buffer.add(\n",
    "                observation=obs,\n",
    "                actor_memory=actor_old.encoder.memory,\n",
    "                critic_memory=critic.encoder.memory\n",
    "            )\n",
    "            if critic.encoder.memory[0].shape[0] == 15:\n",
    "                critic.encoder.reset()\n",
    "\n",
    "            policy = actor_old.get_policy([obs])\n",
    "            action = policy.sample()\n",
    "\n",
    "            buffer.add(action=action, old_logprob=policy.log_prob(action).detach())\n",
    "            \n",
    "            action = action.clone().numpy()\n",
    "            action[:, 1] = (2 * action[:, 1]) - 1 # Move steering action from space [0, 1] to [-1, 1]\n",
    "            action = ActionTuple(action)\n",
    "\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "            buffer.add(reward=reward, is_terminal=done)\n",
    "            \n",
    "            steps += 1\n",
    "\n",
    "            obs = new_obs\n",
    "\n",
    "            if done or steps == N_STEPS: break\n",
    "\n",
    "        actor_memory = actor_old.encoder.memory\n",
    "        critic_memory = critic.encoder.memory\n",
    "\n",
    "        Qval = critic([prepare_obs(obs)]).detach().numpy()\n",
    "        train_ppo(buffer, Qval)\n",
    "\n",
    "        if steps == N_STEPS: break\n",
    "\n",
    " \n",
    "    s = np.sum(rewards)\n",
    "    all_rewards.append(s)\n",
    "    tepisodes.set_postfix(reward=s, len=steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(actor, \"actor\", 2080)\n",
    "save_model(critic, \"critic\", 2080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "done = False\n",
    "\n",
    "actor.eval()\n",
    "\n",
    "obs = env.reset()\n",
    "actor.encoder.reset()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "while not done:\n",
    "    obs = prepare_obs(obs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        action = actor_old.get_policy([obs]).sample().numpy()\n",
    "        action[0, 1] = 2 * action[0, 1] - 1\n",
    "\n",
    "        action = ActionTuple(action)\n",
    "\n",
    "        obs, _, done, _ = env.step(action)\n",
    "\n",
    "        time.sleep(1 / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b42abd3a98a57899ff054d45593396dddd0fb0fbc7b78a5316bc1f3d5abac2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
